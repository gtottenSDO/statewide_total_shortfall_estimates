---
title: Estimating Colorado's Housing Shortfall
authors:
  - name: Greg Totten & Neal Marquez
    affiliation: Colorado State Demography Office
bibliography: references.bib

html:
  include-in-header:
    - text: |
        <meta name="robots" content="noindex">
---

```{r}
#| label: setup
#| cache: false

library(tidyverse)
library(ipumsr)
library(hudr)
library(duckplyr)
library(tidycensus)
library(gt)
library(srvyr)
library(scales)
```

The current file is a draft working version and should be recognized as such. All discussion and analysis should be considered preliminary and subject to change.

## Defining The Shortfall

In order to estimate the total housing shortfall in Colorado, we must first define the metrics by which we are going to assess the number of housing units the state might be short.

How these metrics are defined can have a significant impact on the resulting analysis. As a highly stylized example to demonstrate this concept let's begin by using two examples to bound our estimates:

1.  Every person currently residing in the state does so inside of a permanent housing unit. In this scenario we would have a relatively low estimate of the total number of necessary housing units - as what would be required is enough units to house the state's unhoused population. In this case the estimate of number housing units might just be the estimate of the number of people in this population. However, this could be reduced by changing our requirements of housing to include any shelter - such as tents as vehicles, which would likely bring the estimated much lower.
2.  Every current US resident who would like to live in Colorado may do so, and they will be able to do so for free. Conversely, in this scenario we might expect an estimate that is quite high, as while some people still may opt not to reside in our beautiful state (perhaps they do not particularly like sun), we could reasonably expect many Americans, perhaps into the hundreds of millions, might opt to spend no money to live in our beautiful state.

In between these two estimates are a range of scenarios that might be indicative of the number of housing units which are necessary - based on the objectives we are trying to determine, and the underlying assumptions about housing preferences which underlie them. In this paper we will examine a variety of methods, based primarily on studies by other researchers, that we can apply to Colorado to determine the estimated housing shortfall in the state, under that method. In this way we will provide not so much a point estimate of the total housing shortfall, but a range of estimates which can be utilized by planners and policy makers based on their discretion with respect to the reasonableness and applicability of each method. In doing so we also hope to plan a clear, concise, explanation of the method, what objective it is attempting to solve for, and the meaning of the estimate within that context.

## Data

Data primarily comes from the most recent American Community Survey ("ACS") one year estimates for Colorado, and data from the Colorado State Demography Office ("SDO"). One year ACS estimates are primarily used as the population of the state is large enough to allow for the use of such estimates. If applying similar methodologies at smaller geography levels (such as county level), it may be necessary to instead use 5 year estimates. Additionally, some methods of deriving estimates, such as by analyzing Public Use Microdata Sample ("PUMS") data may not be possible for all methods. As such many methods determined here may only be applicable at the state level.

## Methodology

The methodology used in this analysis utilizes a methodology that larges combines approaches utilized by Up For Growth Housing and Freddie Mac in respective reports published by each [@upforgrowth; @khater2021; @khater2018]. The approach taken begins by comparing Colorado statewide headship rates follows @upforgrowth by applying headship rates (i.e., the percentage of people in a given age group who are head of households) circa 2000 to the 2023 working age population to determine how many households Colorado would have had in 2023 if the population headed households in a similar manner to 2000. The difference between the actual number of households, and those which would have been formed given the earlier headship rates constitutes the shortage attributable to this change in headship rate patterns, while an additional adjustment is then made for changes in vacancy rates among all households during the same period. Where this study differs, however, is that the headship rates are adjusted for changes in the cost burden ratio (${Housing Cost}\over{Income}$) also occurring during the same period, in a method drawing inspiration from that of @khater2021.

::: callout-note
Neal - please feel free to adjust language above - especially around characterization of the adjustments made for cost burden ratio.
:::

## Analysis

PUMS data is first extracted using the IPUMS USA database [@ruggles2024a] via the `ipumsr` package in R [@ipumsr].

```{r}
#| label: load_data_ipums

acs_samples <- get_sample_info("usa") |>
  filter(str_detect(name, pattern = "(2000|2023)a$")) |>
  pull(name)

ipums_file <- list()
ipums_file$base <- "usa_00092"
ipums_file$dir <- "data/ipums_raw/"
ipums_file$loc <- paste0(
  ipums_file$dir,
  ipums_file$base
)

ipums_file$csv <- paste0(ipums_file$loc, ".csv.gz")
ipums_file$ddi <- paste0(ipums_file$loc, ".xml")
ipums_file$json <- paste0(ipums_file$loc, ".json")

if (!file.exists(ipums_file$ddi)) {
  ipums_extract <- define_extract_micro(
    collection = "usa",
    description = "Most recent ACS 1-year samples for Colorado",
    samples = acs_samples,
    data_structure = "hierarchical",
    variables = list(
      var_spec("statefip", case_selections = "08"),
      'year',
      'sample',
      'serial',
      'cbserial',
      'hhwt',
      'cluster',
      'strata',
      'gq',
      'age',
      'ownershp',
      'vacancy',
      'owncost',
      'rent',
      'hhincome',
      'pernum',
      'perwt',
      "repwt",
      "repwtp"
    )
  ) |>
    submit_extract() |>
    wait_for_extract() |>
    download_extract(download_dir = ipums_file$dir)
}

ipums_ddi <- read_ipums_ddi(ipums_file$ddi, lower_vars = TRUE)
# create ipums_vars from the DDI which includes variable information
ipums_vars <- ipums_var_info(ipums_ddi)

ipums_extract <- read_ipums_micro_list(ipums_ddi, var_attrs = NULL) |>
  map(as_duckdb_tibble)

```

### Headship Rate Calculations

Headship Rates are first calculated for 2000 and 2023, based on the PUMS data for each year, grouped by age group[^Additionally these are grouped into AMI and cost burdened designations for potential additional analysis]. The headship rate is calculated as the number of households with a head of household in age group $i$ divided by the total number of households in age group $i$, as provided in @eq-headship-rate. These rates are only calculated for households headed by persons under the age of 65, as changes in headship rates for persons 65 and older are likely determined more by age demographic factors, rather than for economic reasons related to affordability.

$$
hr_{it} = {{hh_{it}}\over{pop_{it}}}
$$ {#eq-headship-rate}

The theoretical motivation behind this approach is that high housing costs may be causing people who would otherwise form their own household to instead "double up", either by continuing to live with a parent or other relative, or by renting a room with roommates. If housing were more available they might instead live on their own. Based on this premise in a more affordable housing market we would expect higher levels of headship rates, as more people in a given age group would then be heading households themselves, rather than living with others for economic reasons.

Recognizing that affordability is not the only determinant of headship rates (for example, the rates at which people might be living with a spouse or partner), we additionally control for headship rates by utilizing relative cost burden (calculated as ${total housing costs}\over{total household income}$) as an additional control, allowing us to attempt to better isolate the effect of housing costs on headship rates.

::: {.callout-note}
Neal - I will need you to elaborate some on this - especially in the context of the analysis
:::

Freddie Mac determines the Target Housing Stock `k*` as a function of target number of households `hh*`, and target vacancy rate `v*`, as provided in [@eq-freddie-mac-housing-units; @khater2021].

$$
k^* = {{hh^*}\over{1-v^*}}
$$ {#eq-freddie-mac-housing-units}

Target households are based on target headship rates according to the methods used in the 2018 Freddie Mac analysis. This analysis calculates target households based on 5 year age groups from the 1994-2018 Current Population Survey-Annual Social and Economic Supplement for target number of households $hh^*$, based on population $pop_i$ and headship rate $hr_i^*$ as provided in [@eq-freddie-mac-headship-rate; @khater2018].

$$ 
{hh^*}= \sum_{i=15}^{65+}pop_i^*hr_i^*
$$ {#eq-freddie-mac-headship-rate}

In their analysis the target headship rate is adjusted based on factors for housing costs, income, and employment, adjusting for the contribution of each using a Oaxaca-Blinder decomposition to determine the relative contribution of each factor. [@khater2018]

In this analysis we simplify the target headship rate to be the average headship rate of the population from 2000-2023, based on our dataset above. We calculate these headship rates by similarly bucketing households based on 5 year age groups from 15-19 to 65+ and calculating the headship rate for each group, where headship rate in year $t$, $hr_{it}$, is calculated as the number of households with a head of household in age group $i$ divided by the total number of households in age group $i$, as provided in @eq-headship-rate. We then calculate the average headship rate for each group across the years 2000-2023, which is used as the target headship rate.

$$
hr_{it} = {{hh_{it}}\over{pop_{it}}}
$$ {#eq-headship-rate}

```{r}
#| label: process-acs-hr

# Calculate headship rates for each age group
acs_data <- ipums_extract$PERSON |>
  # join gq data from household level to person level
  left_join(
    # join household data
    acs_hh_data |>
      select(year, serial, gq, hhincome, ownershp, rent, owncost) |>
      left_join(
        # join mfi data
        tibble(
          year = c(2000, 2023),
          mfi = c(54900, 114500)
        ),
        by = c("year")
      ),
    by = c("year", "serial")
  ) |>
  filter(gq %in% c(1, 2, 5)) |>
  # add number of people in household based on number of person records
  add_count(serial, name = 'hhsize') |>
  # create age bins
  mutate(
    # generate age breaks for cuts
    age_bin = cut(
      age,
      breaks = c(15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, Inf),
      right = FALSE
    ),
    # flag head of household
    head_of_household = ifelse(pernum == 1, 1, 0),
    # add age groups
    age_bin_large = cut(
      age,
      breaks = c(
        0,
        18,
        25,
        45,
        65,
        Inf
      ),
      labels = c(
        "Under 18",
        "18-24",
        "25-44",
        "45-64",
        "65 and over"
      ),
      right = FALSE,
      ordered_result = TRUE
    ),
    # add household size adjustment factor for assigning ami group
    hhadj = case_when(
      hhsize < 4 ~ 1 - (4 - hhsize) * .1,
      hhsize > .4 ~ 1 + (hhsize - 4) * .08,
      .default = 1
    ),
    # calculate applicable ami for household
    mfi_hh = mfi * hhadj,
    # calculate hh percentage of ami
    pct_mfi = pmax(hhincome, 1) / mfi_hh * 100,
    ami_group = cut(
      pct_mfi,
      breaks = c(
        0,
        30,
        60,
        80,
        100,
        Inf
      ),
      right = FALSE,
      include.lowest = TRUE
    ),
    housing_cost_pct = case_when(
      ownershp == 2 ~ rent * 12 / pmax(hhincome, 1),
      ownershp == 1 ~ owncost * 12 / pmax(hhincome, 1),
      .default = NA_real_
    ) *
      100,
    cost_burdened_level = cut(
      housing_cost_pct,
      breaks = c(0, 30, 50, Inf),
      right = FALSE,
      include.lowest = TRUE
    )
  )


acs_hr_srvy <- acs_data |>
  # create survey object from acs_data dataframe
  as_survey_design(
    weight = perwt,
    repweights = matches("repwtp[0-9]+"),
    type = "ACS",
    mse = TRUE
  )

#  verify hhsizes all match highest pernum record value
# acs_data |>
#   group_by(serial, hhsize) |>
#   summarize(max_per = max(pernum)) |>
#   mutate(check = hhsize - max_per) |>
#   filter(check != 0)
```

```{r}
#| label: calc-acs-hr

# calculate headship rates for each age group for each year from 2000-2023
acs_hr_totl_persons <- acs_hr_srvy |>
  survey_count(
    year,
    age_bin,
    ami_group,
    cost_burdened_level,
    name = "total_persons"
  )

acs_hr_hh_heads <- acs_hr_srvy |>
  filter(pernum == 1 & gq %in% c(1, 3, 5)) |>
  survey_count(
    year,
    age_bin,
    ami_group,
    cost_burdened_level,
    name = "total_hh"
  )

acs_headship_rates <- acs_hr_hh_heads |>
  left_join(
    acs_hr_totl_persons,
    by = c("year", "age_bin", "ami_group", "cost_burdened_level")
  ) |>
  mutate(hr = total_hh / total_persons) |>
  group_by(age_bin, ami_group, cost_burdened_level) |>
  mutate(target_hr = hr[year == 2000]) |>
  ungroup() |>
  mutate(
    target_hh = target_hr * total_persons,
    missing_hh = target_hh - total_hh
  ) %>%
  bind_rows(
    summarize(
      .,
      age_bin = "total",
      across(
        c(
          total_hh:total_persons_se,
          target_hh,
          missing_hh
        ),
        sum
      ),
      hr = total_hh / total_persons,
      target_hr = target_hh / total_persons,
      .by = year
    )
  )


```

Using this analysis we find that the target number of households based on target headship rates is extremely sensitive to the end year of the target period. To demonstrate this we run the analysis based on end years of 2010, 2015, and 2020, showing the results for the "total" age_bin.

```{r}
#| label: tbl-headship-rates-ye
#| tbl-cap: "Headship Rates by Final year of Averaging"

headship_rates_by_end_year <-
  map(c(2010, 2015, 2020), calc_headship_rates) |>
  bind_rows() |>
  filter(age_bin == "total" & year == 2023) |>
  select(-year, -age_bin) |>
  gt() |>
  fmt_number(!end_year, decimals = 0) |>
  fmt_percent(hr:target_hr) |>
  cols_align("center") |>
  cols_label(
    end_year = "End year",
    total_persons = "Total Persons",
    total_hh = "Total Households",
    target_hh = "Target Households",
    missing_hh = "Missing Households",
    hr = "Headship Rate",
    target_hr = "Target Headship Rate",
    total_hh_se = "Standard Error",
    total_persons_se = "Standard Error"
  )

headship_rates_by_end_year
```

This is largely driven by the large increase in headship rates beginning around 2006, as seen in @fig-headship-rates.

```{r}
#| label: fig-headship-rates
#| fig-cap: "Headship Rates Over Time"

acs_headship_rates |>
  filter(age_bin == "total") %>%
  bind_rows(
    expand_grid(
      year = 2000:2023,
      summarize(
        .,
        age_bin = "avg",
        hr = mean(hr)
      )
    )
  ) |>
  ggplot(
    aes(
      x = year,
      y = hr,
      color = age_bin
    )
  ) +
  geom_line() +
  scale_y_continuous(
    labels = label_percent()
  ) +
  labs(
    x = "year",
    y = "Headship Rate",
    color = "Overall Headship Rate"
  ) +
  scale_color_discrete(
    labels = c(
      "avg" = "2000-2023 Average",
      "total" = "Annual"
    )
  )
```


Vacancy rates are calculated based on both the `ownershp` and `vacancy` variables. Additionally the `gq` variable is used to filter out group quarters.

In order to calculate vacancy rates it is necessary to first understand what each of the values for each variable represents

```{r}
#| label: ipums_variable_labels

# generate table with variable information
# table will include columns {VAR}_val, and {VAR}_lbl with the value and
# label for each variable. This is setup so tab_spanner_delim can be used to
# separate out columns in a GT table
ipums_var_tbl <- ipums_vars |>
  # only include necessary variables
  filter(var_name %in% c('ownershp', 'vacancy', "gq")) |>
  # remove other columns
  select(var_name, val_labels) |>
  # unnest labels from nested columns
  unnest(val_labels) |>
  # group by var_name for generating row numbers to unpivot without lists
  group_by(var_name) |>
  # add row_numbers
  mutate(row = row_number()) |>
  ungroup() |>
  # pivot wider to generate table
  pivot_wider(
    names_from = var_name,
    values_from = c(val, lbl),
    # specify order of variable then val/lbl column
    names_glue = "{var_name}_{.value}",
    # use slowest so variables are grouped together
    names_vary = "slowest"
  ) |>
  # remove row index as no longer necessary
  select(-row) |>
  # move gq to end
  relocate(starts_with("gq"), .after = last_col())

# create GT table with information
ipums_var_tbl_gt <- ipums_var_tbl |>
  gt() |>
  # create spanners based on the column separators defined above
  tab_spanner_delim(delim = "_") |>
  # Replace NA values with empty strings
  sub_missing(
    missing_text = ""
  )
```

```{r}
#| label: tbl-pums-vars
#| tbl-cap: "Values and respective labels for each of the `ownershp`, `vacancy`, and `gq` variables."

# display table
ipums_var_tbl_gt

```

@tbl-pums-vars includes the values and respective labels for each of the `ownershp`, `vacancy`, and `gq` variables.

The N/A labels associated with `val=0` for the `ownershp` and `vacancy` variables is reflective of units which are Vacant, or Occupied, respectively.

Given these values we then need to establish filtering parameters for determining vacancy rates.

1.  `gq <= 2 OR gq ==5` filter out gq units to only included Households and Vacant Units (including with the 1990 and 2000 Additional Unit definitions)[^1].
2.  `vacancy <= 2` Remove occasional/seasonal vacant units (4-6) consistent with NAHB analysis. Remove migrant farm worker usage (7), and other vacant (9), as these should be relatively independent of overall vacancy rates. Reclassify `3` - Rented or sold but not (yet) occupied to `0` as occupied.
3.  Leave Ownership as is.

[^1]: Note to self - review these definitions to see if we want to exclude any of these groupings.

In order to broadly analyze survey data, and additionally calculate margin of error, the `srvyr` package is used. Prior to analysis a survey object must be defined based on the extracted ACS data. This survey object filters gq units based on condition 1 above, and removes vacant units based on condition 2 above. Additionally, it reclassifies `vacancy == 3` to `0` as occupied. Finally, `N/A` values are relabeled for each of the ownershp, and vacancy variables to reflect that these reflect Vacant and Occupied units, respectively.

```{r}
#| label: calc-vacancy

# create survey design object
acs_hh_data <- ipums_extract$HOUSEHOLD |>
  # remove gq units and vacant units 4-6
  filter(
    gq %in% c(0, 1, 2, 5) & !(vacancy %in% c(4:6, 7, 9))
  ) |>
  # reclassify 3 to 0 and 4-6 to 3
  mutate(
    vacancy = if_else(vacancy == 3, 0, vacancy)
  )

acs_hh_srvy <- acs_hh_data |>
  as_survey_design(
    weight = hhwt,
    repweights = matches("repwt[0-9]+"),
    type = "ACS",
    mse = TRUE
  )

```

Then total up on an annual basis the total occupied units each for sale and for rent, as well as vacant units by for sale or for rent[^2][^3]. We additionally calculate total occupied and total vacant as the sum of each of these. Vacancy rates are then calculated as $\sum vacant_{ot} \over {\sum vacant_{ot}+ \sum occupied_{ot}}$ for each occupancy status $o$ and year $t$. Finally we calculate the sum total of occupied and vacant units for each year[^4].

[^2]: For this purpose we make the assumption that `vacancy == 1` (For rent or sale) is for Rent, and `vacancy ==2` (For sale only) is for sale in order to the simplify the analysis.

[^3]: Margin of error are calculated as 1.645 \* standard error consistent with 90% CI\

[^4]: Note: the Total shortfall is less than the sum of the own and vacant shortfalls due to differences as vacancy rates are determined by tenure.

Once the vacancy rates are calculated, we then calculate the average vacancy rate for each year. To get an annual shortfall we then take the difference between the average vacancy rate and the current vacancy rate. Applying this difference to the total number of units for each year gives us the annual shortfall.

In combining vacancy rates a `tenure` variable is created to determine if a unit is determined to be sale or rental for the purpose of calculating vacancy rates. This is set equal to the value `rent` for `ownershp == 2` and `vacancy == 1`, and `own` for `ownershp== 1` and `vacancy == 2`; consistent with the classifications determined above.

```{r}
#| label: calc-nahb-vacancy

# calculate vacancy
acs_vacancy <- acs_hh_srvy |>
  group_by(vacancy, year) |>
  survey_count(vacancy, name = "vac") |>
  mutate(
    vac_moe = vac_se * 1.645,
    tenure = case_when(
      vacancy == 1 ~ "rent",
      vacancy == 2 ~ "own"
    )
  ) |>
  filter(!is.na(tenure)) |>
  ungroup()

# calculate occupied
acs_occupancy <- acs_hh_srvy |>
  group_by(ownershp, year) |>
  survey_count(ownershp, name = "occ") |>
  mutate(
    occ_moe = occ_se * 1.645,
    tenure = case_when(
      ownershp == 1 ~ "own",
      ownershp == 2 ~ "rent"
    )
  ) |>
  filter(!is.na(tenure)) |>
  ungroup()

# calculate total occupied and total vacant
acs_combined_ov <- acs_vacancy |>
  select(year, tenure, vac, vac_moe) |>
  left_join(
    acs_occupancy |>
      select(year, tenure, occ, occ_moe),
    by = c("year", "tenure")
  ) %>% # old style pipe needs to be used to pipe . into bind_rows
  bind_rows(
    summarize(
      .,
      tenure = "total",
      across(c(vac:occ_moe), sum),
      .by = year
    )
  ) |>
  mutate(
    total = vac + occ,
    vac_rate = vac / total
  ) |>
  group_by(tenure) |>
  mutate(avg_vac_rate = mean(vac_rate)) |>
  ungroup() |>
  mutate(
    diff_from_avg = avg_vac_rate - vac_rate,
    shortfall = diff_from_avg * total
  )

current_shortfall <- acs_combined_ov |>
  filter(year == 2023) |>
  select(-year) |>
  mutate(tenure = str_to_title(tenure)) |>
  gt() |>
  fmt_number(c(vac:total, shortfall), decimals = 0) |>
  fmt_percent(c(vac_rate:diff_from_avg)) |>
  cols_label(
    tenure = "Tenure",
    vac = "Vacant",
    vac_moe = "Margin of Error",
    occ = "Occupied",
    occ_moe = "Margin of Error",
    total = "Total",
    vac_rate = "vacancy Rate",
    avg_vac_rate = "Average vacancy Rate",
    diff_from_avg = "Difference from Average",
    shortfall = "Shortfall"
  ) |>
  cols_align("center")

```

```{r}
#| label: tbl-nahb-shortfall
#| tbl-cap: "Current shortfall of occupied housing units by tenure type using NAHB Methodology."

current_shortfall

# define get_shortfall function to return shortfall based on tenure and format data
extract_shortfall <- function(ten_type) {
  current_shortfall |>
    fmt_number(decimals = 1, suffixing = "K") |>
    extract_cells(shortfall, rows = tenure == ten_type)
}

```

@tbl-nahb-shortfall contains the calculated shortfall for 2023 using the NAHB methodology. Using this methodology shortfalls of `r extract_shortfall("Rent")` units for rent and `r extract_shortfall("Own")` units for ownership are determined, for a total shortfall of `r extract_shortfall("Total")`[^5].

[^5]: Note: the Total shortfall is less than the sum of the own and vacant shortfalls due to differences as vacancy rates are determined by tenure.
