---
title: Estimating Colorado's Housing Shortfall
authors:
  - name: Greg Totten & Neal Marquez
    affiliation: Colorado State Demography Office
bibliography: references.bib

html:
  include-in-header:
    - text: |
        <meta name="robots" content="noindex">
---

```{r}
#| label: setup
#| cache: false

library(tidyverse)
library(ipumsr)
library(hudr)
library(duckplyr)
library(tidycensus)
library(gt)
library(srvyr)
library(scales)
```

The current file is a draft working version and should be recognized as such. All discussion and analysis should be considered preliminary and subject to change.

## Defining The Shortfall

In order to estimate the total housing shortfall in Colorado, we must first define the metrics by which we are going to assess the number of housing units the state might be short.

How these metrics are defined can have a significant impact on the resulting analysis. As a highly stylized example to demonstrate this concept let's begin by using two examples to bound our estimates:

1.  Every person currently residing in the state does so inside of a permanent housing unit. In this scenario we would have a relatively low estimate of the total number of necessary housing units - as what would be required is enough units to house the state's unhoused population. In this case the estimate of number housing units might just be the estimate of the number of people in this population. However, this could be reduced by changing our requirements of housing to include any shelter - such as tents as vehicles, which would likely bring the estimated much lower.
2.  Every current US resident who would like to live in Colorado may do so, and they will be able to do so for free. Conversely, in this scenario we might expect an estimate that is quite high, as while some people still may opt not to reside in our beautiful state (perhaps they do not particularly like sun), we could reasonably expect many Americans, perhaps into the hundreds of millions, might opt to spend no money to live in our beautiful state.

In between these two estimates are a range of scenarios that might be indicative of the number of housing units which are necessary - based on the objectives we are trying to determine, and the underlying assumptions about housing preferences which underlie them. In this paper we will examine a variety of methods, based primarily on studies by other researchers, that we can apply to Colorado to determine the estimated housing shortfall in the state, under that method. In this way we will provide not so much a point estimate of the total housing shortfall, but a range of estimates which can be utilized by planners and policy makers based on their discretion with respect to the reasonableness and applicability of each method. In doing so we also hope to plan a clear, concise, explanation of the method, what objective it is attempting to solve for, and the meaning of the estimate within that context.

## Data

Data primarily comes from the most recent American Community Survey ("ACS") one year estimates for Colorado, and data from the Colorado State Demography Office ("SDO"). One year ACS estimates are primarily used as the population of the state is large enough to allow for the use of such estimates. If applying similar methodologies at smaller geography levels (such as county level), it may be necessary to instead use 5 year estimates. Additionally, some methods of deriving estimates, such as by analyzing Public Use Microdata Sample ("PUMS") data may not be possible for all methods. As such many methods determined here may only be applicable at the state level.

## Methodology

The methodology used in this analysis utilizes a methodology that larges combines approaches utilized by Up For Growth Housing and Freddie Mac in respective reports published by each [@upforgrowth; @khater2021; @khater2018]. The approach taken begins by comparing Colorado statewide headship rates follows @upforgrowth by applying headship rates (i.e., the percentage of people in a given age group who are head of households) circa 2000 to the 2023 working age population to determine how many households Colorado would have had in 2023 if the population headed households in a similar manner to 2000. The difference between the actual number of households, and those which would have been formed given the earlier headship rates constitutes the shortage attributable to this change in headship rate patterns, while an additional adjustment is then made for changes in vacancy rates among all households during the same period. Where this study differs, however, is that the headship rates are adjusted for changes in the cost burden ratio (${Housing Cost}\over{Income}$) also occurring during the same period, in a method drawing inspiration from that of @khater2021.

::: callout-note
Neal - please feel free to adjust language above - especially around characterization of the adjustments made for cost burden ratio.
:::

The first step is to create a time series of vacancy rates for the state with ACS data. This data is accessed from the IPUMS USA database [@ruggles2024a] using the `ipumsr` package in R [@ipumsr].

```{r}
#| label: load_data_ipums

acs_samples <- get_sample_info("usa") |>
  filter(str_detect(name, pattern = "(2000|2023)a$")) |>
  pull(name)

ipums_file <- list()
ipums_file$base <- "usa_00092"
ipums_file$dir <- "data/ipums_raw/"
ipums_file$loc <- paste0(
  ipums_file$dir,
  ipums_file$base
)

ipums_file$csv <- paste0(ipums_file$loc, ".csv.gz")
ipums_file$ddi <- paste0(ipums_file$loc, ".xml")
ipums_file$json <- paste0(ipums_file$loc, ".json")

if (!file.exists(ipums_file$ddi)) {
  ipums_extract <- define_extract_micro(
    collection = "usa",
    description = "Most recent ACS 1-year samples for Colorado",
    samples = acs_samples,
    data_structure = "hierarchical",
    variables = list(
      var_spec("statefip", case_selections = "08"),
      'year',
      'sample',
      'serial',
      'cbserial',
      'hhwt',
      'cluster',
      'strata',
      'gq',
      'age',
      'ownershp',
      'vacancy',
      'owncost',
      'rent',
      'hhincome',
      'pernum',
      'perwt',
      "repwt",
      "repwtp"
    )
  ) |>
    submit_extract() |>
    wait_for_extract() |>
    download_extract(download_dir = ipums_file$dir)
}

ipums_ddi <- read_ipums_ddi(ipums_file$ddi, lower_vars = TRUE)
# create ipums_vars from the DDI which includes variable information
ipums_vars <- ipums_var_info(ipums_ddi)

ipums_extract <- read_ipums_micro_list(ipums_ddi, var_attrs = NULL) |>
  map(as_duckdb_tibble)

```

```{r}
#| label: load_data_hud

```

Vacancy rates are calculated based on both the `ownershp` and `vacancy` variables. Additionally the `gq` variable is used to filter out group quarters.

In order to calculate vacancy rates it is necessary to first understand what each of the values for each variable represents

```{r}
#| label: ipums_variable_labels

# generate table with variable information
# table will include columns {VAR}_val, and {VAR}_lbl with the value and
# label for each variable. This is setup so tab_spanner_delim can be used to
# separate out columns in a GT table
ipums_var_tbl <- ipums_vars |>
  # only include necessary variables
  filter(var_name %in% c('ownershp', 'vacancy', "gq")) |>
  # remove other columns
  select(var_name, val_labels) |>
  # unnest labels from nested columns
  unnest(val_labels) |>
  # group by var_name for generating row numbers to unpivot without lists
  group_by(var_name) |>
  # add row_numbers
  mutate(row = row_number()) |>
  ungroup() |>
  # pivot wider to generate table
  pivot_wider(
    names_from = var_name,
    values_from = c(val, lbl),
    # specify order of variable then val/lbl column
    names_glue = "{var_name}_{.value}",
    # use slowest so variables are grouped together
    names_vary = "slowest"
  ) |>
  # remove row index as no longer necessary
  select(-row) |>
  # move gq to end
  relocate(starts_with("gq"), .after = last_col())

# create GT table with information
ipums_var_tbl_gt <- ipums_var_tbl |>
  gt() |>
  # create spanners based on the column separators defined above
  tab_spanner_delim(delim = "_") |>
  # Replace NA values with empty strings
  sub_missing(
    missing_text = ""
  )
```

```{r}
#| label: tbl-pums-vars
#| tbl-cap: "Values and respective labels for each of the `ownershp`, `vacancy`, and `gq` variables."

# display table
ipums_var_tbl_gt

```

@tbl-pums-vars includes the values and respective labels for each of the `ownershp`, `vacancy`, and `gq` variables.

The N/A labels associated with `val=0` for the `ownershp` and `vacancy` variables is reflective of units which are Vacant, or Occupied, respectively.

Given these values we then need to establish filtering parameters for determining vacancy rates.

1.  `gq <= 2 OR gq ==5` filter out gq units to only included Households and Vacant Units (including with the 1990 and 2000 Additional Unit definitions)[^1].
2.  `vacancy <= 2` Remove occasional/seasonal vacant units (4-6) consistent with NAHB analysis. Remove migrant farm worker usage (7), and other vacant (9), as these should be relatively independent of overall vacancy rates. Reclassify `3` - Rented or sold but not (yet) occupied to `0` as occupied.
3.  Leave Ownership as is.

[^1]: Note to self - review these definitions to see if we want to exclude any of these groupings.

In order to broadly analyze survey data, and additionally calculate margin of error, the `srvyr` package is used. Prior to analysis a survey object must be defined based on the extracted ACS data. This survey object filters gq units based on condition 1 above, and removes vacant units based on condition 2 above. Additionally, it reclassifies `vacancy == 3` to `0` as occupied. Finally, `N/A` values are relabeled for each of the ownershp, and vacancy variables to reflect that these reflect Vacant and Occupied units, respectively.

```{r}
#| label: calc-vacancy

# create survey design object
acs_hh_data <- ipums_extract$HOUSEHOLD |>
  # remove gq units and vacant units 4-6
  filter(
    gq %in% c(0, 1, 2, 5) & !(vacancy %in% c(4:6, 7, 9))
  )
# reclassify 3 to 0 and 4-6 to 3
# mutate(
#   vacancy = lbl_relabel(
#     vacancy,
#     0 ~ .val == 3
#   ),
#   ownershp = lbl_relabel(
#     ownershp,
#     lbl(0, "Vacant") ~ .val == 0
#   )
# # ) |>
# as_factor(levels = "values") |>
# as_duckplyr_tibble()

acs_hh_srvy <- acs_hh_data |>
  as_survey_design(
    weight = hhwt,
    repweights = matches("repwt[0-9]+"),
    type = "ACS",
    mse = TRUE
  )

```

Then total up on an annual basis the total occupied units each for sale and for rent, as well as vacant units by for sale or for rent[^2]\[\^margin of error are calculated as 1.645 \* standard error consistent with 90% CI\]. We additionally calculate total occupied and total vacant as the sum of each of these. Vacancy rates are then calculated as \$ \sum VACANT\_{ot} \over {\\\\\\\\sum VACANT\\\\\\\_{ot}+ \\\\\\\\sum OCCUPIED\\\\\\\_{ot}}\$ for each occupancy status $o$ and year $t$. Finally we calculate the sum total of occupied and vacant units for each year[^3].

[^2]: For this purpose we make the assumption that `vacancy == 1` (For rent or sale) is for Rent, and `vacancy ==2` (For sale only) is for sale in order to the simplify the analysis.

[^3]: Note to self - verify that margin of error are additive

Once the vacancy rates are calculated, we then calculate the average vacancy rate for each year. To get an annual shortfall we then take the difference between the average vacancy rate and the current vacancy rate. Applying this difference to the total number of units for each year gives us the annual shortfall.

In combining vacancy rates a `tenure` variable is created to determine if a unit is determined to be sale or rental for the purpose of calculating vacancy rates. This is set equal to the value `rent` for `OWNSHP == 2` and `vacancy == 1`, and `own` for `OWNSHP == 1` and `vacancy == 2`; consistent with the classifications determined above.

```{r}
#| label: calc-nahb-vacancy

# calculate vacancy
acs_vacancy <- acs_hh_srvy |>
  group_by(vacancy, year) |>
  survey_count(vacancy, name = "vac") |>
  mutate(
    vac_moe = vac_se * 1.645,
    tenure = case_when(
      vacancy == 1 ~ "rent",
      vacancy == 2 ~ "own"
    )
  ) |>
  filter(!is.na(tenure)) |>
  ungroup()

# calculate occupied
acs_occupancy <- acs_hh_srvy |>
  group_by(ownershp, year) |>
  survey_count(ownershp, name = "occ") |>
  mutate(
    occ_moe = occ_se * 1.645,
    tenure = case_when(
      ownershp == 1 ~ "own",
      ownershp == 2 ~ "rent"
    )
  ) |>
  filter(!is.na(tenure)) |>
  ungroup()

# calculate total occupied and total vacant
acs_combined_ov <- acs_vacancy |>
  select(year, tenure, vac, vac_moe) |>
  left_join(
    acs_occupancy |>
      select(year, tenure, occ, occ_moe),
    by = c("year", "tenure")
  ) %>% # old style pipe needs to be used to pipe . into bind_rows
  bind_rows(
    summarize(
      .,
      tenure = "total",
      across(c(vac:occ_moe), sum),
      .by = year
    )
  ) |>
  mutate(
    total = vac + occ,
    vac_rate = vac / total
  ) |>
  group_by(tenure) |>
  mutate(avg_vac_rate = mean(vac_rate)) |>
  ungroup() |>
  mutate(
    diff_from_avg = avg_vac_rate - vac_rate,
    shortfall = diff_from_avg * total
  )

current_shortfall <- acs_combined_ov |>
  filter(year == 2023) |>
  select(-year) |>
  mutate(tenure = str_to_title(tenure)) |>
  gt() |>
  fmt_number(c(vac:total, shortfall), decimals = 0) |>
  fmt_percent(c(vac_rate:diff_from_avg)) |>
  cols_label(
    tenure = "Tenure",
    vac = "Vacant",
    vac_moe = "Margin of Error",
    occ = "Occupied",
    occ_moe = "Margin of Error",
    total = "Total",
    vac_rate = "vacancy Rate",
    avg_vac_rate = "Average vacancy Rate",
    diff_from_avg = "Difference from Average",
    shortfall = "Shortfall"
  ) |>
  cols_align("center")

```

```{r}
#| label: tbl-nahb-shortfall
#| tbl-cap: "Current shortfall of occupied housing units by tenure type using NAHB Methodology."

current_shortfall

# define get_shortfall function to return shortfall based on tenure and format data
extract_shortfall <- function(ten_type) {
  current_shortfall |>
    fmt_number(decimals = 1, suffixing = "K") |>
    extract_cells(shortfall, rows = tenure == ten_type)
}


```

@tbl-nahb-shortfall contains the calculated shortfall for 2023 using the NAHB methodology. Using this methodology shortfalls of `r extract_shortfall("Rent")` units for rent and `r extract_shortfall("Own")` units for ownership are determined, for a total shortfall of `r extract_shortfall("Total")`[^4].

[^4]: Note: the Total shortfall is less than the sum of the own and vacant shortfalls due to differences as vacancy rates are determined by tenure.

### Freddie Mac

Freddie Mac determines the Target Housing Stock `k*` as a function of target number of households `hh*`, and target vacancy rate `v*`, as provided in [@eq-freddie-mac-housing-units; @khater2021].

$$
k^* = {{hh^*}\over{1-v^*}}
$$ {#eq-freddie-mac-housing-units}

Target households are based on target headship rates according to the methods used in the 2018 Freddie Mac analysis. This analysis calculates target households based on 5 year age groups from the 1994-2018 Current Population Survey-Annual Social and Economic Supplement for target number of households $hh^*$, based on population $pop_i$ and headship rate $hr_i^*$ as provided in [@eq-freddie-mac-headship-rate; @khater2018].

$$ 
{hh^*}= \sum_{i=15}^{65+}pop_i^*hr_i^*
$$ {#eq-freddie-mac-headship-rate}

In their analysis the target headship rate is adjusted based on factors for housing costs, income, and employment, adjusting for the contribution of each using a Oaxaca-Blinder decomposition to determine the relative contribution of each factor. [@khater2018]

In this analysis we simplify the target headship rate to be the average headship rate of the population from 2000-2023, based on our dataset above. We calculate these headship rates by similarly bucketing households based on 5 year age groups from 15-19 to 65+ and calculating the headship rate for each group, where headship rate in year $t$, $hr_{it}$, is calculated as the number of households with a head of household in age group $i$ divided by the total number of households in age group $i$, as provided in @eq-headship-rate. We then calculate the average headship rate for each group across the years 2000-2023, which is used as the target headship rate.

$$
hr_{it} = {{hh_{it}}\over{pop_{it}}}
$$ {#eq-headship-rate}

```{r}
#| label: process-acs-hr

# Calculate headship rates for each age group
acs_data <- ipums_extract$PERSON |>
  # join gq data from household level to person level
  left_join(
    # join household data
    acs_hh_data |>
      select(year, serial, gq, hhincome, ownershp, rent, owncost) |>
      left_join(
        # join mfi data
        tibble(
          year = c(2000, 2023),
          mfi = c(54900, 114500)
        ),
        by = c("year")
      ),
    by = c("year", "serial")
  ) |>
  filter(gq %in% c(1, 2, 5)) |>
  # add number of people in household based on number of person records
  add_count(serial, name = 'hhsize') |>
  # create age bins
  mutate(
    # generate age breaks for cuts
    age_bin = cut(
      age,
      breaks = c(15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, Inf),
      right = FALSE
    ),
    # flag head of household
    head_of_household = ifelse(pernum == 1, 1, 0),
    # add age groups
    age_bin_large = cut(
      age,
      breaks = c(
        0,
        18,
        25,
        45,
        65,
        Inf
      ),
      labels = c(
        "Under 18",
        "18-24",
        "25-44",
        "45-64",
        "65 and over"
      ),
      right = FALSE,
      ordered_result = TRUE
    ),
    # add household size adjustment factor for assigning ami group
    hhadj = case_when(
      hhsize < 4 ~ 1 - (4 - hhsize) * .1,
      hhsize > .4 ~ 1 + (hhsize - 4) * .08,
      .default = 1
    ),
    # calculate applicable ami for household
    mfi_hh = mfi * hhadj,
    # calculate hh percentage of ami
    pct_mfi = pmax(hhincome, 1) / mfi_hh * 100,
    ami_group = cut(
      pct_mfi,
      breaks = c(
        0,
        30,
        60,
        80,
        100,
        Inf
      ),
      right = FALSE,
      include.lowest = TRUE
    ),
    housing_cost_pct = case_when(
      ownershp == 2 ~ rent * 12 / pmax(hhincome, 1),
      ownershp == 1 ~ owncost * 12 / pmax(hhincome, 1),
      .default = NA_real_
    ) *
      100,
    cost_burdened_level = cut(
      housing_cost_pct,
      breaks = c(0, 30, 50, Inf),
      right = FALSE,
      include.lowest = TRUE
    )
  )


acs_hr_srvy <- acs_data |>
  # create survey object from acs_data dataframe
  as_survey_design(
    weight = perwt,
    repweights = matches("repwtp[0-9]+"),
    type = "ACS",
    mse = TRUE
  )

#  verify hhsizes all match highest pernum record value
# acs_data |>
#   group_by(serial, hhsize) |>
#   summarize(max_per = max(pernum)) |>
#   mutate(check = hhsize - max_per) |>
#   filter(check != 0)
```

```{r}
#| label: calc-acs-hr

# calculate headship rates for each age group for each year from 2000-2023
acs_hr_totl_persons <- acs_hr_srvy |>
  survey_count(
    year,
    age_bin,
    ami_group,
    cost_burdened_level,
    name = "total_persons"
  )

acs_hr_hh_heads <- acs_hr_srvy |>
  filter(pernum == 1 & gq %in% c(1, 3, 5)) |>
  survey_count(
    year,
    age_bin,
    ami_group,
    cost_burdened_level,
    name = "total_hh"
  )

acs_headship_rates <- acs_hr_hh_heads |>
  left_join(
    acs_hr_totl_persons,
    by = c("year", "age_bin", "ami_group", "cost_burdened_level")
  ) |>
  mutate(hr = total_hh / total_persons) |>
  group_by(age_bin, ami_group, cost_burdened_level) |>
  mutate(target_hr = hr[year == 2000]) |>
  ungroup() |>
  mutate(
    target_hh = target_hr * total_persons,
    missing_hh = target_hh - total_hh
  ) %>%
  bind_rows(
    summarize(
      .,
      age_bin = "total",
      across(
        c(
          total_hh:total_persons_se,
          target_hh,
          missing_hh
        ),
        sum
      ),
      hr = total_hh / total_persons,
      target_hr = target_hh / total_persons,
      .by = year
    )
  )


```

Using this analysis we find that the target number of households based on target headship rates is extremely sensitive to the end year of the target period. To demonstrate this we run the analysis based on end years of 2010, 2015, and 2020, showing the results for the "total" age_bin.

```{r}
#| label: tbl-headship-rates-ye
#| tbl-cap: "Headship Rates by Final year of Averaging"

headship_rates_by_end_year <-
  map(c(2010, 2015, 2020), calc_headship_rates) |>
  bind_rows() |>
  filter(age_bin == "total" & year == 2023) |>
  select(-year, -age_bin) |>
  gt() |>
  fmt_number(!end_year, decimals = 0) |>
  fmt_percent(hr:target_hr) |>
  cols_align("center") |>
  cols_label(
    end_year = "End year",
    total_persons = "Total Persons",
    total_hh = "Total Households",
    target_hh = "Target Households",
    missing_hh = "Missing Households",
    hr = "Headship Rate",
    target_hr = "Target Headship Rate",
    total_hh_se = "Standard Error",
    total_persons_se = "Standard Error"
  )

headship_rates_by_end_year
```

This is largely driven by the large increase in headship rates beginning around 2006, as seen in @fig-headship-rates.

```{r}
#| label: fig-headship-rates
#| fig-cap: "Headship Rates Over Time"

acs_headship_rates |>
  filter(age_bin == "total") %>%
  bind_rows(
    expand_grid(
      year = 2000:2023,
      summarize(
        .,
        age_bin = "avg",
        hr = mean(hr)
      )
    )
  ) |>
  ggplot(
    aes(
      x = year,
      y = hr,
      color = age_bin
    )
  ) +
  geom_line() +
  scale_y_continuous(
    labels = label_percent()
  ) +
  labs(
    x = "year",
    y = "Headship Rate",
    color = "Overall Headship Rate"
  ) +
  scale_color_discrete(
    labels = c(
      "avg" = "2000-2023 Average",
      "total" = "Annual"
    )
  )
```

#### Freddie Mac SDO age Groups

As a follow up we compare the ACS estimates from implied estimates using SDO data from 2010 on, based on the Household Projections created by SDO. Both household and population by age data data is pulled from the SDO Postgres using tables `estimates.household_projections` and `county_census_grouping_ya_region`.

```{r}
#| label: connect_pg_sdo

library(DBI)
library(RPostgres)
# initiate connection to SDO postgres
sdo_postgres <- dbConnect(
  RPostgres::Postgres(),
  dbname = 'dola',
  host = '104.197.26.248',
  port = 5433,
  user = Sys.getenv("SDO_USER_R"), #environment variable with read only user needs to be saved
  password = Sys.getenv("SDO_PW_R"), #environment variable with read only pw needs to be saved
  bigint = 'integer64'
)

connections::connection_view(sdo_postgres)

```

```{r}
#| label: calc_sdo_hr

sdo_hh_proj <- tbl(
  sdo_postgres,
  Id(
    schema = "estimates",
    table = "household_projections"
  )
) |>
  filter(area_code == 0 & household_type_id == 0) |>
  select(
    area_code,
    year,
    age_group_id,
    age_group = age_group_description,
    hh = total_households
  )

sdo_pop <- tbl(
  sdo_postgres,
  Id(
    schema = "estimates",
    table = "county_sya"
  )
) |>
  select(area_code = countyfips, year, age, totalpopulation) |>
  filter(area_code == 0 & age >= 18) |>
  mutate(
    age_group_id = as.integer(cut(
      age,
      breaks = c(18, 24, 44, 64, Inf),
      labels = FALSE,
      include.lowest = TRUE
    ))
  ) |>
  summarize(
    pop = sum(totalpopulation),
    .by = c(area_code, year, age_group_id)
  ) |>
  collect()

sdo_pop <- sdo_pop |>
  bind_rows(
    sdo_pop |>
      summarize(
        age_group_id = 0,
        pop = sum(pop),
        .by = c(area_code, year)
      )
  )

sdo_combined <- sdo_hh_proj |>
  collect() |>
  left_join(
    sdo_pop,
    by = join_by(area_code, year, age_group_id)
  ) |>
  mutate(
    headship_rate = hh / pop
  ) |>
  arrange(area_code, year, age_group_id)
```

Next we recalculate the ACS data using SDO age Groups ("18 to 24", "25 to 44", "45 to 64", "65+") so we can compare against SDO households and headship rates.

```{r}
#| label: calc_hr_acs_sdo

acs_hr_tot_persons_sdo <- acs_hr_srvy |>
  survey_count(year, age_group_id, name = "pop") |>
  bind_rows(
    acs_hr_srvy |>
      survey_count(year, name = "pop") |>
      mutate(age_group_id = 0, .after = year)
  ) |>
  arrange(year, age_group_id)

acs_hr_hh_heads_sdo <- acs_hr_srvy |>
  filter(pernum == 1) |>
  survey_count(year, age_group_id, name = "hh") |>
  bind_rows(
    acs_hr_srvy |>
      filter(pernum == 1) |>
      survey_count(year, name = "hh") |>
      mutate(age_group_id = 0, .after = year)
  ) |>
  arrange(year, age_group_id)

acs_hr_sdo <- acs_hr_tot_persons_sdo |>
  left_join(acs_hr_hh_heads_sdo) |>
  filter(!is.na(age_group_id)) |>
  mutate(
    headship_rate = hh / pop
  ) |>
  rename(year = year)

acs_sdo_combined <- acs_hr_sdo |>
  full_join(
    sdo_combined,
    suffix = c("_acs", "_sdo"),
    by = join_by(year, age_group_id)
  ) |>
  arrange(year, age_group_id) |>
  mutate(
    pop_diff = pop_acs - pop_sdo,
    check_pop = if_else(abs(pop_diff) <= pop_se, TRUE, FALSE)
  ) |>
  select(
    year,
    age_group_id,
    age_group,
    pop_acs,
    pop_se,
    pop_sdo,
    pop_diff,
    check_pop,
    hh_acs,
    hh_se,
    headship_rate_acs,
    headship_rate_sdo
  )

write_csv(acs_sdo_comparison, "acs_sdo_comparison.csv")
```

### National Association of Realtors

This uses the decline in building rates during the 2000s relative to the long term averages between 1968 and 2000 to determine the shortfall in housing from the supply side. This method was not recreated due to the inherent underlying assumptions. In particular, growth rates are not necessarily linear over time, and this is particularly the case with rapidly growing states. Colorado in particular was undergoing rapid urbanization and it is not necessarily safe to assume that growth of this style would continue.

### National Low Income Housing Coalitition (NLIHC)

This calculates the housing gap based on the difference between households by AMI (extremely low-income, very low-income, low-income, middle-income, or above median income), based on the applicable HUD AMI. Housing Units were separately categorized into income necessary to rent them, based on spending more than 30% of their income on housing costs (regardless of the actual income of the household occupying that housing unit). Finally they examined the extent to which households in each AMI group resided in a unit which was affordable to that household. [@thegap2024]

While this methodology is similar to the method SDO is currently working as part of its Housing Needs Assessments there is no need to recreate the table directly, as NLIHC provides state level results for all states, including [Colorado](https://nlihc.org/gap/state/co), based on 2022 ACS data.

Based on their methodology NLIHC finds Colorado to be short 119,782 housing units at the Extremely Low Income (0 to 30% of Area Median Income), and 165,053 units for households at or below 50% AMI (including those who were classified as Extremely Low Income. While statewide they find that there are 101 Affordable and Available Rental Units per 100 Households for those At or Below 100% AMI, there are only 27 and 44 units Affordable and Available to those at or below the 30% and 50% AMI levels, respectively. Similarly they find that for households which are in the 0-30%, and 31-50% AMI bands, 88% and 83% units, respectively are Cost Burdened, while only 22% of units between 81 and 100% AMI are. [@gaprepo]

One important footnote to the NLIHC study, however, is that the units identified are not necessarily new housing units which must be built. Rather they indicate households that may reside in housing units already, but for which the cost may be unaffordable. This distinction is important because the housing already exists, and the objective then is to determine a method for making these units affordable to these families, which can occur in different ways, including by subsidizing rents for residents to bridge the gap between 30% of their income and the market rent being charged on their unit. Additionally, because housing prices are determined on the margins, each additional housing unit in theory pushes down housing prices by some incremental amount, holding demand equal. Therefore the number of units necessary to push prices down to a level which becomes affordable to residents at these costs is not necessarily equal to the total number of households experience affordability problems, and, in fact, may be significantly higher or lower depending on these effects and the manner to which lower rents filter down through the housing stock.